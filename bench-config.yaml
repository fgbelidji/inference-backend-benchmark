
model: &model Qwen/Qwen3-0.6B

benchmark: 
  dataset-name: "random"
  request-rate: 1
  random-input-len: 1024
  random-output-len: 128
  num-prompts: 10


backends:
  vllm:
    image: vllm/vllm-openai:latest
    port: 8000
    args:
      - --model 
      - *model
      - --dtype 
      - bfloat16
      - --max_model_len 
      - 4096
      - --gpu_memory_utilization 
      - 0.9
      - --tensor_parallel_size 
      - 1
      - --data_parallel_size 
      - 1

  sglang:
    image: lmsysorg/sglang:dev
    port: &port 30000
    args:
      - python3 -m sglang.launch_server
      - --host 
      - 0.0.0.0
      - --port
      - *port
      - --model-path 
      - *model
      - --dtype 
      - bfloat16
      - --tensor-parallel-size 
      - 4
      - --data-parallel-size 
      - 1
      - --context-length 
      - 4096
      - --disable-cuda-graph
      - --mem-fraction-static
      - 0.7
      - --disable-overlap-schedule
      - --enable-p2p-check

  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    port: 3000
    args:

      - --model-id
      - *model
      - --dtype 
      - bfloat16
      - --num-shard 
      - 4
      - --max-input-tokens 
      - 4096
      - --port 
      - 3000


